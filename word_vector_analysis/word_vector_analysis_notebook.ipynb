{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab4ded2c",
   "metadata": {},
   "source": [
    "# Word Vector Analysis Project\n",
    "\n",
    "In this project, we explore word vectors derived from the word2vec algorithm, examining the relationships between words captured via these vectors. We will also use them as features for a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f86c75",
   "metadata": {},
   "source": [
    "## Project Task\n",
    "\n",
    "This project involves using pre-computed word vectors to perform tasks such as finding similar words, solving word analogies, and leveraging these vectors to improve a classifier model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46fd2ce",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a99de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import uniform\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# To ensure consistency in outputs\n",
    "RANDOM_SEED = 655"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d8ead3",
   "metadata": {},
   "source": [
    "### Part 1: Loading the Word Vectors\n",
    "\n",
    "#### Task 1.1: Load Pre-computed Word Vectors\n",
    "\n",
    "Pre-computed word vectors are stored in a dictionary `wiki_wv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3b6cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wv = {}\n",
    "with open(r'C:/users/akama/downloads/word_vector_analysis/wiki_vocab.txt', encoding='utf-8') as f:\n",
    "    wiki_wv['vocab'] = f.read().split('\\n')\n",
    "wiki_wv['wv'] = np.load(r'C:/users/akama/downloads/word_vector_analysis/wiki_vects.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209d4606",
   "metadata": {},
   "source": [
    "#### Basic Facts about Our Word Vectors\n",
    "\n",
    "Assign the following variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "631958ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134282 words and 100 dimensions\n"
     ]
    }
   ],
   "source": [
    "n_words = len(wiki_wv['wv'])\n",
    "n_dims = wiki_wv['wv'].shape[1]\n",
    "print(f'{n_words} words and {n_dims} dimensions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b2ee9",
   "metadata": {},
   "source": [
    "#### Task 1.2: Construct a Word Index\n",
    "\n",
    "Create a dictionary `word_index` to map words to their indices in the word vector array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbacad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = dict(zip(wiki_wv['vocab'], range(len(wiki_wv['vocab']))))\n",
    "wiki_wv['index'] = word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f646b1c",
   "metadata": {},
   "source": [
    "#### Task 1.2.1: Get the Vector for a Given Word\n",
    "\n",
    "Complete the `get_word_vector` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "178e704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word, wv=wiki_wv):\n",
    "    word_idx = wv['index'].get(word)    \n",
    "    if word_idx is not None:\n",
    "        return wv['wv'][word_idx]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c24039c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.4823521e-01  2.3072057e+00  2.1295850e+00 -9.1626394e-01\n",
      "  1.0018171e+00  1.4503113e+00  4.5253047e-01  1.4185722e+00\n",
      "  1.6953593e+00  1.0559855e+00  2.7143070e-01  1.5554521e+00\n",
      " -3.4456417e-01  1.9864979e-01  2.7903378e+00 -8.6733478e-01\n",
      " -4.2806253e-01  3.4738421e-01 -1.2050803e+00  1.0622426e+00\n",
      " -5.6063598e-01  5.9032774e-01 -1.0257747e+00 -7.6530254e-01\n",
      "  3.3861476e-01 -8.8240725e-01  7.9443592e-01 -1.3805602e+00\n",
      " -1.2598097e+00  9.5285571e-01 -1.9514867e+00 -2.8805381e-01\n",
      "  2.0856528e-01  2.3167922e+00  1.9959958e-01  2.1145422e+00\n",
      "  2.2699206e-01  6.3021503e-02  8.1352192e-01  2.5985492e-03\n",
      "  1.8291645e+00  1.1634727e+00 -8.0728352e-01  8.4317046e-01\n",
      " -1.2693784e-01 -1.6598600e-01  2.2009730e+00  8.4855229e-01\n",
      "  2.9602451e+00 -1.3164682e+00 -5.9475186e-03 -3.0840275e-01\n",
      " -1.9252799e-01  1.5872755e+00  1.2728233e+00  1.1041660e+00\n",
      " -5.9441972e-01  1.4613307e+00 -6.2290657e-01  1.7193762e+00\n",
      " -5.8605254e-01 -2.7472922e-01 -2.0975404e+00  1.6103998e+00\n",
      "  2.7877734e+00 -1.6588722e+00 -8.5985637e-01  1.2236402e+00\n",
      "  1.5724623e+00 -1.6611415e+00  1.1111791e-01 -3.4276760e-01\n",
      " -1.9729739e-01  6.7898536e-01  6.9372332e-01 -9.2601317e-01\n",
      " -1.0491383e+00  3.9250795e-02  6.5078074e-01 -8.6706465e-01\n",
      " -6.2666786e-01  1.7486613e+00  1.8594164e-01 -2.5307276e+00\n",
      " -1.1819793e+00 -2.6645293e+00 -2.3320867e-01  1.3281034e+00\n",
      "  7.0965415e-01 -6.7614783e-03 -3.2585299e-01 -1.9352386e+00\n",
      "  9.6055156e-01 -6.4331645e-01 -5.4109973e-01  1.0142511e-01\n",
      " -1.6205037e+00 -9.8446184e-01 -1.0234348e+00 -1.5978404e+00]\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "the_wiki_vect = get_word_vector('the')\n",
    "print(the_wiki_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e81ec16",
   "metadata": {},
   "source": [
    "### Part 2: Examine What's Represented by the Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eedaba5",
   "metadata": {},
   "source": [
    "### Task 2.1: Get Similar Words\n",
    "\n",
    "Define the `get_most_similar function` to find words similar to a given word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03bed7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_cossims(vect, wv=wiki_wv):\n",
    "    return cosine_similarity(np.array([vect]), wv['wv']).flatten()\n",
    "\n",
    "def get_most_similar(word, wv=wiki_wv, k=10):\n",
    "    word_vector = get_word_vector(word, wv)\n",
    "    if word_vector is None:\n",
    "        return []\n",
    "    cossims = get_vector_cossims(word_vector, wv)\n",
    "    word_idx = wv['index'].get(word)\n",
    "    if word_idx is not None:\n",
    "        cossims[word_idx] = -1\n",
    "    most_similar_indices = np.argsort(cossims)[-k:][::-1]\n",
    "    most_similar_words = [wv['vocab'][idx] for idx in most_similar_indices]\n",
    "    return most_similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56f2edd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar to biologist:\n",
      "geneticist\n",
      "biochemist\n",
      "physicist\n",
      "microbiologist\n",
      "physiologist\n",
      "paleontologist\n",
      "geophysicist\n",
      "virologist\n",
      "zoologist\n",
      "neuroscientist\n",
      "===\n",
      "\n",
      "most similar to France:\n",
      "Belgium\n",
      "Spain\n",
      "Algeria\n",
      "Italy\n",
      "Marseille\n",
      "Portugal\n",
      "Morocco\n",
      "Bordeaux\n",
      "Brazil\n",
      "Switzerland\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "print('most similar to biologist:')\n",
    "for word in get_most_similar('biologist'):\n",
    "    print(word)\n",
    "print('===\\n')\n",
    "print('most similar to France:')\n",
    "for word in get_most_similar('France'):\n",
    "    print(word)\n",
    "print('===')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c3ae7",
   "metadata": {},
   "source": [
    "#### Task 2.2: Examine Word Analogies\n",
    "\n",
    "Define the `get_analogy` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24fa137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(vect):\n",
    "    return vect / np.linalg.norm(vect)\n",
    "\n",
    "def get_analogy(a, b, c, wv=wiki_wv):\n",
    "    a_vec = get_word_vector(a, wv)\n",
    "    b_vec = get_word_vector(b, wv)\n",
    "    c_vec = get_word_vector(c, wv)\n",
    "    if a_vec is None or b_vec is None or c_vec is None:\n",
    "        return None\n",
    "    norm_a = normalize(a_vec)\n",
    "    norm_b = normalize(b_vec)\n",
    "    norm_c = normalize(c_vec)\n",
    "    d_prime = norm_b - norm_a + norm_c\n",
    "    cossims = get_vector_cossims(d_prime, wv)\n",
    "    for word in [a, b, c]:\n",
    "        word_idx = wv['index'][word]\n",
    "        cossims[word_idx] = -1\n",
    "    most_similar_idx = np.argmax(cossims)\n",
    "    most_similar_word = wv['vocab'][most_similar_idx]\n",
    "    return most_similar_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7a08417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London\n",
      "chemistry\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "print(get_analogy('France','Paris','England'))\n",
    "print(get_analogy('biologist', 'biology', 'chemist'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe7e11",
   "metadata": {},
   "source": [
    "#### Task 2.2.1: Test Word Vector Analogies on Professions\n",
    "\n",
    "Evaluate the performance of word vector analogies on different professions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "249a4aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: archaeologist; output: archaeology\n",
      "input: botanist; output: botany\n",
      "input: economist; output: economics\n",
      "input: entomologist; output: botany\n",
      "input: linguist; output: mathematics\n",
      "input: mathematician; output: mathematics\n",
      "input: oncologist; output: oncology\n",
      "input: physicist; output: physics\n",
      "input: statistician; output: microbiology\n",
      "input: zoologist; output: botany\n"
     ]
    }
   ],
   "source": [
    "test_professions = ['archaeologist', 'botanist', 'economist', 'entomologist', 'linguist', 'mathematician', 'oncologist', \n",
    "                    'physicist', 'statistician', 'zoologist']\n",
    "for profession in test_professions:\n",
    "    print('input: %s; output: %s' % (profession, get_analogy('biologist', 'biology', profession)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc0e9a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entomologist', 'linguist', 'statistician', 'zoologist']\n"
     ]
    }
   ],
   "source": [
    "# Identify incorrect outputs\n",
    "\n",
    "wrong_professions = ['entomologist', 'linguist', 'statistician', 'zoologist']\n",
    "print(wrong_professions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662747ca",
   "metadata": {},
   "source": [
    "#### Task 2.2.2: Test Word Vector Analogies on Countries and Cities\n",
    "\n",
    "Evaluate the performance of word vector analogies on different countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "247eb0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: Austria; output: Vienna\n",
      "input: Belgium; output: Brussels\n",
      "input: Canada; output: Toronto\n",
      "input: China; output: Shanghai\n",
      "input: Germany; output: Berlin\n",
      "input: India; output: Calcutta\n",
      "input: Japan; output: Tokyo\n",
      "input: Portugal; output: Lisbon\n",
      "input: Spain; output: Madrid\n",
      "input: Tanzania; output: Nairobi\n",
      "['Canada', 'China', 'India', 'Tanzania']\n"
     ]
    }
   ],
   "source": [
    "test_countries = ['Austria', 'Belgium', 'Canada', 'China', 'Germany', 'India', 'Japan', 'Portugal', 'Spain', 'Tanzania']\n",
    "results = {}\n",
    "for country in test_countries:\n",
    "    result = get_analogy('France', 'Paris', country)\n",
    "    results[country] = result\n",
    "    print('input: %s; output: %s' % (country, result))\n",
    "\n",
    "expected_capitals = {\n",
    "    'Austria': 'Vienna',\n",
    "    'Belgium': 'Brussels',\n",
    "    'Canada': 'Ottawa',\n",
    "    'China': 'Beijing',\n",
    "    'Germany': 'Berlin',\n",
    "    'India': 'New Delhi',\n",
    "    'Japan': 'Tokyo',\n",
    "    'Portugal': 'Lisbon',\n",
    "    'Spain': 'Madrid',\n",
    "    'Tanzania': 'Dodoma'\n",
    "}\n",
    "\n",
    "wrong_countries = [country for country, output in results.items() if output.lower() != expected_capitals[country].lower()]\n",
    "print(wrong_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6288c",
   "metadata": {},
   "source": [
    "### Part 3: Use Word Vectors as Classifier Features\n",
    "\n",
    "#### Task 3.1.1: Filter out Infrequent Labels\n",
    "\n",
    "Filter the rows in `nationality_df` for labels occurring at least 500 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec8f5154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51931 19\n"
     ]
    }
   ],
   "source": [
    "nationality_df = pd.read_csv(r'C:/users/akama/downloads/word_vector_analysis/bio_name_nationality.tsv.gz', \n",
    "                             sep='\\t', compression='gzip')\n",
    "nationality_df = nationality_df.dropna()\n",
    "nationality_df = nationality_df[:75000]\n",
    "MIN_NATIONALITY_COUNT = 500\n",
    "def standardize_nationality(nationality):\n",
    "    parts = nationality.split(',')\n",
    "    standardized_label = parts[-1].strip()\n",
    "    return standardized_label\n",
    "\n",
    "nationality_df['nationality'] = nationality_df['nationality'].apply(standardize_nationality)\n",
    "cleaned_nationality_df = nationality_df[nationality_df.groupby('nationality')['nationality'].\n",
    "                                        transform('count')>=MIN_NATIONALITY_COUNT]\n",
    "print(len(cleaned_nationality_df), cleaned_nationality_df.nationality.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060f406",
   "metadata": {},
   "source": [
    "#### Task 3.1.2: Create Train/Dev/Test Data Splits\n",
    "\n",
    "Split the cleaned data into train, development, and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02066c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41544 5193 5194\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SIZE = .8\n",
    "DEV_SIZE = .1\n",
    "TEST_SIZE = .1\n",
    "\n",
    "train_dev_df, test_df = train_test_split(\n",
    "    cleaned_nationality_df, test_size=TEST_SIZE, random_state=RANDOM_SEED, shuffle=True\n",
    ")\n",
    "\n",
    "train_proportion_in_train_dev = TRAIN_SIZE / (TRAIN_SIZE + DEV_SIZE)\n",
    "\n",
    "train_df, dev_df = train_test_split(\n",
    "    train_dev_df, test_size=(1 - train_proportion_in_train_dev), random_state=RANDOM_SEED, shuffle=True\n",
    ")\n",
    "\n",
    "print(len(train_df), len(dev_df), len(test_df))\n",
    "\n",
    "y_train = list(train_df.nationality)\n",
    "y_dev = list(dev_df.nationality)\n",
    "y_test = list(test_df.nationality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa2967",
   "metadata": {},
   "source": [
    "#### Task 3.2: Tokenize Text and Remove Stopwords\n",
    "\n",
    "Tokenize each biography and remove stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "974c6803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 1220\n"
     ]
    }
   ],
   "source": [
    "stop_words = ENGLISH_STOP_WORDS\n",
    "tokenized_train_items = []\n",
    "tokenized_dev_items = []\n",
    "\n",
    "stop_words = set(ENGLISH_STOP_WORDS)\n",
    "token_pattern = re.compile(r'(?u)\\b\\w\\w+\\b')\n",
    "\n",
    "def tokenize_and_remove_stopwords(text, token_pattern, stop_words):\n",
    "    tokens = token_pattern.findall(text)\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "tokenized_train_items = [tokenize_and_remove_stopwords(bio, token_pattern, stop_words) for bio in train_df['bio']]\n",
    "tokenized_dev_items = [tokenize_and_remove_stopwords(bio, token_pattern, stop_words) for bio in dev_df['bio']]\n",
    "\n",
    "print(len(tokenized_train_items[0]), len(tokenized_dev_items[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03862c3",
   "metadata": {},
   "source": [
    "#### Task 3.3: Compute Word Vector-Based Features\n",
    "\n",
    "Compute the mean word vector for each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f6c4eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 100)\n",
      "[[ 0.39517793 -0.07868216 -0.92518407 ...  0.15139848 -0.08917703\n",
      "   0.43999135]\n",
      " [-0.22381873 -0.78749293  0.40706831 ...  0.41614151 -0.24410585\n",
      "   0.23181398]\n",
      " [ 0.08421799 -0.16369714 -0.53750986 ...  0.86370862  0.90777248\n",
      "   0.47205114]\n",
      " ...\n",
      " [ 0.13395222 -0.50426126  0.24911262 ... -0.08104277  0.00547608\n",
      "   0.99351317]\n",
      " [-0.09968048 -0.68094653  0.14089933 ... -0.46044263 -0.25196463\n",
      "   0.33508641]\n",
      " [ 0.58683908  0.43996775 -0.965339   ... -0.31179458  0.26025856\n",
      "  -0.47247902]]\n"
     ]
    }
   ],
   "source": [
    "def generate_word_vector_features(tokenized_texts, wv=wiki_wv):\n",
    "    vocab = set(wv['vocab'])\n",
    "    word_vectors = wv['wv']\n",
    "    index = wv['index']\n",
    "    features = np.zeros((len(tokenized_texts), word_vectors.shape[1]))\n",
    "    for i, tokens in enumerate(tokenized_texts):\n",
    "        valid_vectors = [word_vectors[index[token]] for token in tokens if token in vocab]\n",
    "        if valid_vectors:\n",
    "            mean_vector = np.mean(valid_vectors, axis=0)\n",
    "        else:\n",
    "            mean_vector = np.zeros(word_vectors.shape[1])\n",
    "        features[i] = mean_vector\n",
    "    return features\n",
    "\n",
    "# Test with a small subset\n",
    "X_sample = generate_word_vector_features(tokenized_train_items[:200], wiki_wv)\n",
    "print(X_sample.shape)\n",
    "print(X_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd9e428",
   "metadata": {},
   "source": [
    "#### Task 3.3.1: Compute Features for the Entire Data\n",
    "\n",
    "Generate word-vector-based features for the train and dev sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e76ca52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41544, 100) (5193, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train_wv = generate_word_vector_features(tokenized_train_items, wiki_wv)\n",
    "X_dev_wv = generate_word_vector_features(tokenized_dev_items, wiki_wv)\n",
    "print(X_train_wv.shape, X_dev_wv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48532da",
   "metadata": {},
   "source": [
    "#### Task 3.4: Train and Evaluate Classifier\n",
    "\n",
    "Train a Logistic Regression classifier and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e53be7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END ..........................clf__C=0.8003453423381894; total time=   7.1s\n",
      "[CV] END ..........................clf__C=0.8003453423381894; total time=   4.2s\n",
      "[CV] END ..........................clf__C=0.8003453423381894; total time=   4.6s\n",
      "[CV] END ..........................clf__C=2.7509546782812846; total time=   4.7s\n",
      "[CV] END ..........................clf__C=2.7509546782812846; total time=   5.2s\n",
      "[CV] END ..........................clf__C=2.7509546782812846; total time=   5.5s\n",
      "[CV] END ...........................clf__C=3.734248334862691; total time=   5.1s\n",
      "[CV] END ...........................clf__C=3.734248334862691; total time=   4.9s\n",
      "[CV] END ...........................clf__C=3.734248334862691; total time=   6.1s\n",
      "[CV] END ..........................clf__C=0.5574663215139735; total time=   3.8s\n",
      "[CV] END ..........................clf__C=0.5574663215139735; total time=   4.2s\n",
      "[CV] END ..........................clf__C=0.5574663215139735; total time=   4.2s\n",
      "[CV] END .........................clf__C=0.48861782325626324; total time=   4.2s\n",
      "[CV] END .........................clf__C=0.48861782325626324; total time=   3.9s\n",
      "[CV] END .........................clf__C=0.48861782325626324; total time=   4.0s\n",
      "[CV] END ...........................clf__C=3.311879157453186; total time=   5.4s\n",
      "[CV] END ...........................clf__C=3.311879157453186; total time=   6.4s\n",
      "[CV] END ...........................clf__C=3.311879157453186; total time=   6.8s\n",
      "[CV] END ...........................clf__C=3.717603977132823; total time=   6.5s\n",
      "[CV] END ...........................clf__C=3.717603977132823; total time=   5.9s\n",
      "[CV] END ...........................clf__C=3.717603977132823; total time=   6.9s\n",
      "[CV] END ...........................clf__C=2.986570658025275; total time=   6.1s\n",
      "[CV] END ...........................clf__C=2.986570658025275; total time=   6.1s\n",
      "[CV] END ...........................clf__C=2.986570658025275; total time=   6.5s\n",
      "[CV] END ...........................clf__C=1.977326075029889; total time=   5.6s\n",
      "[CV] END ...........................clf__C=1.977326075029889; total time=   5.6s\n",
      "[CV] END ...........................clf__C=1.977326075029889; total time=   6.0s\n",
      "[CV] END ...........................clf__C=1.202947021711163; total time=   6.0s\n",
      "[CV] END ...........................clf__C=1.202947021711163; total time=   6.1s\n",
      "[CV] END ...........................clf__C=1.202947021711163; total time=   5.3s\n",
      "[CV] END ..........................clf__C=1.1155275996506964; total time=   4.7s\n",
      "[CV] END ..........................clf__C=1.1155275996506964; total time=   4.8s\n",
      "[CV] END ..........................clf__C=1.1155275996506964; total time=   5.1s\n",
      "[CV] END ...........................clf__C=1.116510848234121; total time=   5.3s\n",
      "[CV] END ...........................clf__C=1.116510848234121; total time=   5.0s\n",
      "[CV] END ...........................clf__C=1.116510848234121; total time=   5.0s\n",
      "[CV] END ...........................clf__C=3.869378005313089; total time=   6.1s\n",
      "[CV] END ...........................clf__C=3.869378005313089; total time=   5.7s\n",
      "[CV] END ...........................clf__C=3.869378005313089; total time=   6.8s\n",
      "[CV] END ...........................clf__C=2.334894812117859; total time=   5.1s\n",
      "[CV] END ...........................clf__C=2.334894812117859; total time=   5.2s\n",
      "[CV] END ...........................clf__C=2.334894812117859; total time=   6.0s\n",
      "[CV] END ..........................clf__C=3.2788184957495554; total time=   5.2s\n",
      "[CV] END ..........................clf__C=3.2788184957495554; total time=   5.9s\n",
      "[CV] END ..........................clf__C=3.2788184957495554; total time=   5.7s\n",
      "[CV] END ..........................clf__C=1.7336951134815308; total time=   4.9s\n",
      "[CV] END ..........................clf__C=1.7336951134815308; total time=   5.0s\n",
      "[CV] END ..........................clf__C=1.7336951134815308; total time=   5.3s\n",
      "[CV] END ........................clf__C=0.016467752445364692; total time=   2.7s\n",
      "[CV] END ........................clf__C=0.016467752445364692; total time=   2.6s\n",
      "[CV] END ........................clf__C=0.016467752445364692; total time=   2.6s\n",
      "[CV] END ..........................clf__C=2.0785936804021667; total time=   5.5s\n",
      "[CV] END ..........................clf__C=2.0785936804021667; total time=   5.2s\n",
      "[CV] END ..........................clf__C=2.0785936804021667; total time=   5.7s\n",
      "[CV] END .........................clf__C=0.03014598658490275; total time=   2.9s\n",
      "[CV] END .........................clf__C=0.03014598658490275; total time=   2.9s\n",
      "[CV] END .........................clf__C=0.03014598658490275; total time=   3.0s\n",
      "[CV] END ..........................clf__C=0.6816715881259974; total time=   4.7s\n",
      "[CV] END ..........................clf__C=0.6816715881259974; total time=   4.7s\n",
      "[CV] END ..........................clf__C=0.6816715881259974; total time=   4.8s\n",
      "0.725333629547167\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  \n",
    "    ('clf', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000, solver='lbfgs', n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_dist = {\n",
    "    'clf__C': uniform(loc=0, scale=4),\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=20, cv=3, scoring='f1_macro', n_jobs=None, random_state=RANDOM_SEED, verbose=2)\n",
    "random_search.fit(X_train_wv, y_train)\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred_dev = best_model.predict(X_dev_wv)\n",
    "lr_wv_f1 = f1_score(y_dev, y_pred_dev, average='macro')\n",
    "\n",
    "print(lr_wv_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c01dd",
   "metadata": {},
   "source": [
    "#### Task 3.5: Consider Model Size\n",
    "\n",
    "Compute the number of feature weights required for both tf-idf and word vector features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c63f642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num feature weights for tfidf 52440\n",
      "num feature weights for word vects 1900\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=500, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(train_df.bio)\n",
    "num_tfidf_features = X_train.shape[1]\n",
    "num_wv_features = X_train_wv.shape[1]\n",
    "num_classes = len(set(y_train))\n",
    "num_tfidf_feature_weights = num_tfidf_features * num_classes\n",
    "num_wv_feature_weights = num_wv_features * num_classes\n",
    "print('num feature weights for tfidf', num_tfidf_feature_weights)\n",
    "print('num feature weights for word vects', num_wv_feature_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
